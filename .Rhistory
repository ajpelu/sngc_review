# Metlting data (to matrix)
meltdf <- melt(df_loc)
aux_df <- acast(meltdf, variable~taxon, value.var = 'value')
# Compute richness and evenness
Riqueza <- specnumber(aux_df)
Evenness <- diversity(aux_df) / log(Riqueza)
# Combine results into an object
y <- as.data.frame(t(rbind(Riqueza,Evenness)))
# Compute summary statistics
aux <- y %>% summarise_each(funs(mean, sd, min, max, length, se=sd(.)/sqrt(n()))) %>%
gather(variable, value) %>%
separate(variable, c("var", "stat"), sep = "\\_") %>%
spread(stat, value)
aux$loc <- j
aux$replica <- i
output <- rbind(output, aux)}}
output
riqueza <- output %>% separate(loc, c('localidad','Trat'), sep=1) %>%
mutate(localidad = ifelse(localidad=='C', 'Canar', 'San Jeronimo')) %>%
mutate(Trat = ifelse(Trat %in% c(0,30,50,100), Trat,
substring(Trat, 2))) %>%
filter(var=='Riqueza') %>%
select(-c(max, min)) %>%
group_by(localidad, Trat) %>%
summarise(riq=mean(mean, na.rm = TRUE),
n=length(mean),
sd=sd(mean, na.rm = TRUE),
se=sd/sqrt(n))
View(riqueza)
output %>% separate(loc, c('localidad','Trat'), sep=1) %>%
mutate(localidad = as.factor(ifelse(localidad=='C', 'Canar', 'San Jeronimo'))) %>%
filter(var=='Riqueza') %>%
select(-c(max, min, Trat, replica, length, sd, se)) %>%
select(media = mean, localidad) %>%
group_by(localidad)
output %>% separate(loc, c('localidad','Trat'), sep=1) %>%
mutate(localidad = as.factor(ifelse(localidad=='C', 'Canar', 'San Jeronimo'))) %>%
filter(var=='Riqueza') %>%
select(-c(max, min, Trat, replica, length, sd, se)) %>%
select(media = mean, localidad) %>%
group_by(localidad) %>% summarise(riq=mean(media, na.rm = TRUE),
n=length(media),
sd=sd(media, na.rm = TRUE),
se=sd/sqrt(n))
riqueza_loc <- output %>% separate(loc, c('localidad','Trat'), sep=1) %>%
mutate(localidad = as.factor(ifelse(localidad=='C', 'Canar', 'San Jeronimo'))) %>%
filter(var=='Riqueza') %>%
select(-c(max, min, Trat, replica, length, sd, se)) %>%
select(media = mean, localidad) %>%
group_by(localidad) %>% summarise(riq=mean(media, na.rm = TRUE),
n=length(media),
sd=sd(media, na.rm = TRUE),
se=sd/sqrt(n))
riqueza_loc
install.packages("neotoma")
library(neotoma)
sn <- get_site(sitename = 'Sierra Nev*%')
sn <- get_site(sitename = 'Sierra Nev*%')
sn <- get_site(sitename = 'Gra*%')
sn
detach("package:datasets", unload=TRUE)
di <- '/Users/ajpelu/ownCloud/MS/MS_BIENESTAR_USO_CARO/datos_old_ms/'
## Leer datos
data <- read.csv(file=paste(di,'data/datos750.csv', sep=''), head=TRUE, sep=",", dec=".")
library('p2distance') # calculo indicador
library('xlsx') # Exportar a excel
library('plyr') # manejo de datos
library('sm') # density
library('epicalc') # qqplots, etc.
library('ggplot2')
library('boot') # bootstrap
library('Hmisc') # varios: bootstrap
library('data.table')
library('gplots')
library('multcomp')
library('gridExtra')
library('party')
source(paste(di,'R/AUXILIAR/summarySE.R', sep=''))
source(paste(di,'R/AUXILIAR/bootdif.R', sep=''))
source(paste(di,'R/AUXILIAR/effectSize.R', sep=''))
########### Packages necesarios #############
nrow(data[data$year==1989,])
nrow(data[data$year==2009,])
# is different
# What cod values are in 1989 and there aren't in 2009?
cod1989 <- data[data$year==1989, c('cod','year')]
cod2009 <- data[data$year==2009, c('cod','year')]
noestan <- cod2009[!(cod2009$cod %in% cod1989$cod),]
## The codes 11901 and 11902 aren't in 1989
data <- data[!(data$cod==11901 & data$year==2009),]
dataF <- data[!(data$cod==11902 & data$year==2009),]
# Check again
nrow(dataF[dataF$year==1989,])
nrow(dataF[dataF$year==2009,])
View(dataF)
write.table(dataF, file=paste(di, 'data/datos_749.csv', sep=''), sep=";", dec=".")
write.table(dataF, file=paste(di, 'data/datos_749.csv', sep=''), sep=",", dec=".")
write.table(dataF, file=paste(di, 'data/datos_749.csv', sep=''), sep=",", dec=".")
# Select only partial indicators to compute dp2. I selected also entity
matriz.datos <- dataF[,c(4:26)]
# Rename rowname with entity value
entidades <- matriz.datos[,1]
matriz.datos <- matriz.datos[-1]
row.names(matriz.datos) <- entidades
# Convert into matrix object (Note: To improve in p2distance package)
matriz.datos <- as.matrix(matriz.datos)
# If reference_vector=NULL (i.e; default value), p2distance use the minimun as reference vector (see p2distane)
p2d <- p2distance(matriz.datos, reference_vector=NULL, iterations=50)
###### EXPLORE P2DISTACE RESULTS
# It reaches convergence after three iterations
head(p2d$diff_p2distances)
# Create an output file with results
# Variables order
aux.data <- dataF[,c('cod','year','entity')]
indicador <- data.frame(p2d$p2distance, entity=rownames(p2d$p2distance))
bienestar <- merge(aux.data, indicador, by='entity')
names(bienestar)[4] <- 'dp2'
bienestar$year <- as.factor(bienestar$year)
View(bienestar)
r89 <- i89[,c('cod','dp2')]
i89 <- bienestar[bienestar$year==1989,]
i09 <- bienestar[bienestar$year==2009,]
r89 <- i89[,c('cod','dp2')]
names(r89)[2] <- 'dp2.89'
r09 <- i09[,c('cod','dp2')]
names(r09)[2] <- 'dp2.09'
ratio <- join(r09, r89, by='cod')
ratio$rat <- ratio$dp2.09/ratio$dp2.89
write.xlsx(ratio, file=paste(di,'data/ratio.xlsx', sep=''),
sheetName='ratios', showNA=FALSE, append=FALSE, row.names=TRUE)
write.table(ratio, file=paste(di,'data/ratio.csv', sep=''), sep=",", dec=".")
View(ratio)
View(dataF)
di <- '/Users/ajpelu/ownCloud/MS/MS_BIENESTAR_USO_CARO/datos_old_ms/'
## Leer datos
data <- read.csv(file=paste(di,'data/datos750.csv', sep=''), head=TRUE, sep=",", dec=".")
###################################################################
####################### LOAD PACKAGES #############################
library('p2distance') # calculo indicador
library('xlsx') # Exportar a excel
library('plyr') # manejo de datos
library('sm') # density
library('epicalc') # qqplots, etc.
library('ggplot2')
library('boot') # bootstrap
library('Hmisc') # varios: bootstrap
library('data.table')
library('gplots')
library('multcomp')
library('gridExtra')
library('party')
source(paste(di,'R/AUXILIAR/summarySE.R', sep=''))
source(paste(di,'R/AUXILIAR/bootdif.R', sep=''))
source(paste(di,'R/AUXILIAR/effectSize.R', sep=''))
nrow(data[data$year==1989,])
nrow(data[data$year==2009,])
# is different
# What cod values are in 1989 and there aren't in 2009?
cod1989 <- data[data$year==1989, c('cod','year')]
cod2009 <- data[data$year==2009, c('cod','year')]
noestan <- cod2009[!(cod2009$cod %in% cod1989$cod),]
## The codes 11901 and 11902 aren't in 1989
data <- data[!(data$cod==11901 & data$year==2009),]
dataF <- data[!(data$cod==11902 & data$year==2009),]
# Check again
nrow(dataF[dataF$year==1989,])
nrow(dataF[dataF$year==2009,])
# OCT 2015
# Exportar tabla con 749 municipios
write.table(dataF, file=paste(di, 'data/datos_749.csv', sep=''), sep=",", dec=".", row.names = FALSE)
################# COMPUTE DP2 #####################################
# Select only partial indicators to compute dp2. I selected also entity
matriz.datos <- dataF[,c(4:26)]
# Rename rowname with entity value
entidades <- matriz.datos[,1]
matriz.datos <- matriz.datos[-1]
row.names(matriz.datos) <- entidades
# Convert into matrix object (Note: To improve in p2distance package)
matriz.datos <- as.matrix(matriz.datos)
# If reference_vector=NULL (i.e; default value), p2distance use the minimun as reference vector (see p2distane)
p2d <- p2distance(matriz.datos, reference_vector=NULL, iterations=50)
###### EXPLORE P2DISTACE RESULTS
# It reaches convergence after three iterations
head(p2d$diff_p2distances)
aux.data <- dataF[,c('cod','year','entity')]
# p2distance indicator
indicador <- data.frame(p2d$p2distance, entity=rownames(p2d$p2distance))
# Join data
bienestar <- merge(aux.data, indicador, by='entity')
names(bienestar)[4] <- 'dp2'
bienestar$year <- as.factor(bienestar$year)
i89 <- bienestar[bienestar$year==1989,]
i09 <- bienestar[bienestar$year==2009,]
r89 <- i89[,c('cod','dp2')]
names(r89)[2] <- 'dp2.89'
r09 <- i09[,c('cod','dp2')]
names(r09)[2] <- 'dp2.09'
ratio <- join(r09, r89, by='cod')
ratio$rat <- ratio$dp2.09/ratio$dp2.89
write.table(ratio, file=paste(di,'data/ratio.csv', sep=''), sep=",", dec=".", row.names = FALSE)
quit()
quit()
di <- '/Users/ajpelu/ownCloud/MS/DOSSIER2013_FICHAS/NIEVE/ts_snow_dossier/'
library('wq')
library('reshape2')
install.packages("wq")
indicadores <- c('scd', 'scod', 'scmd', 'scmc' )
for (j in indicadores) {
# read data
aux <- read.csv(file=paste(di, '/data/', j, '.csv', sep=''), header=T, sep=';')
# change variable names
names(aux) <- c('sen_slope','sen_slope_pct', 'p_value', 'S', 'varS', 'miss', 'tau', 'nie_malla_modi_id')
# Classify the p_value
aux$sig <- ifelse(aux$p_value <= 0.05, 'sig', 'no sig')
# Add a column with variable name
aux$variable <- rep(j, nrow(aux))
# Assign output to an dataframe
assign(paste('raw_',j,sep=''), aux)
# remove aux object
rm(aux)
}
View(raw_scd)
elev <- read.csv(file=paste(di,'/data/geoinfo/elevation.csv', sep=''), head=TRUE, sep=",", dec=".")
# Join data of elevation with indicators
elev_indicators <- elev[which(elev$nie_malla_modi_id %in% raw_scd$nie_malla_modi_id), ]
scd <- join(raw_scd, elev_indicators, type='inner', , by='nie_malla_modi_id')
scod <- join(raw_scod, elev_indicators, type='inner', , by='nie_malla_modi_id')
scmd <- join(raw_scmd, elev_indicators, type='inner', , by='nie_malla_modi_id')
scmc <- join(raw_scmc, elev_indicators, type='inner', , by='nie_malla_modi_id')
library('reshape2')
library('plyr')
library('ggplot2')
library('gdata') # reorder factors
library('devtools')
scd <- join(raw_scd, elev_indicators, type='inner', , by='nie_malla_modi_id')
scod <- join(raw_scod, elev_indicators, type='inner', , by='nie_malla_modi_id')
scmd <- join(raw_scmd, elev_indicators, type='inner', , by='nie_malla_modi_id')
scmc <- join(raw_scmc, elev_indicators, type='inner', , by='nie_malla_modi_id')
View(scd)
View(scd)
View(scmc)
mydf <- rbind(scd, scod, scmd)
library(dplyr)
View(scd)
mydf <- mydf %>% select(elev > 1900)
str(mydf)
mydf <- mydf %>% select(filter > 1900)
mydf <- mydf %>% filter(elev > 1900)
View(mydf)
names(mydf)
mydf <- mydf %>% select(sen_slope, p_value, tau, nie_malla_modi_id, sig, variable, elev) %>% filter(elev > 1900)
mydf <- rbind(scd, scod, scmd)
# Filter by 1900 meters
mydf <- mydf %>% select(sen_slope, p_value, tau, nie_malla_modi_id, sig, variable, elev) %>% filter(elev > 1900)
View(mydf)
read.csv(file=paste(di, '/data/geoinfo/latlong_snow.csv', sep=''), header=T, sep=';')
coord <- read.csv(file=paste(di, '/data/geoinfo/latlong_snow.csv', sep=''), header=T, sep=';')
View(coord)
names(coord)
names(coord) <- c('nie_malla_modi_id', 'st_x', 'st_y')
mydf2 <- join(mydf, coord, type='inner', by='nie_malla_modi_id')
View(mydf2)
write.table(mydf2, file=paste(di, '/data/nieveSN2000_2014.csv', sep=''), row.names=FALSE, sep=',')
di <- '/Users/ajpelu/Desktop/datos_visualizacion/'
# Load packages and customized functions
library('reshape2')
library('plyr')
library('ggplot2')
library('gdata') # reorder factors
library('dplyr')
ndvi <- read.csv(file=paste(di,'ontologia_tendencia_ndvi.csv', sep=''), head=TRUE, sep=",", dec=".")
snow <- read.csv(file=paste(di,'ontologia_tendencia_nieve.csv', sep=''), head=TRUE, sep=",", dec=".")
View(snow)
names(snow)
names(ndvi)
View(ndvi)
quit
quit()
library(devtools)
devtools::install_github("DataONEorg/rdataone/dataonelibs", ref="D1_CLIENT_R_v1.0.0")
devtools::install_github("DataONEorg/rdataone/dataone", ref="D1_CLIENT_R_v1.0.0")
library(datone)
library(dataone)
cli <- D1Client()
d1o <- getD1Object(cli, "doi:10.5063/AA/hstuar01.10.1")
mydf <- asDataFrame(d1o)
View(mydf)
d1o <- getD1Object(cli, "doi:10.5063/F1ZP441N")
mydf <- asDataFrame(d1o)
View(mydf)
View(snow)
0.737+0.315+0.495
0.315 + 0.495 + 0.775
1.084+ 1.296+ 1.134
0.775+ 1.084+ 1.296
1.134+0.507+ 1.052
1.125+0.575+ 1.164
0.737+ 0.315 +0.495
5.56 - 4.52
library(slidify)
install.packages("slidify")
pkgs <- c('ramnathv/slidifyLibraries', 'ramnathv/slidify')
devtools::install_github(pkgs)
getwd()
x <- read.table(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt")
x <- read.csv(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt")
x <- read.csv(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt", header=TRUE)
x <- read.csv(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt", header=TRUE, sep='\t')
x <- read.txt(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt", header=TRUE, sep='\t')
x <- read.table(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt", header=TRUE, sep='\t')
x <- read.table(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_Tab.txt", sep='\t')
x <- read.csv(file="/Users/ajpelu/Downloads/busqueda_test.csv", sep='@@@@')
x <- read.table(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_FTab.txt", sep='\t')
x <- read.table(file="/Users/ajpelu/Downloads/Busqueda_19_02_16_FTab.txt", header=TRUE, sep='\t')
x <- read.table(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500.txt",
header = TRUE,
sep = '\t')
x <- read.table(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500.txt",
header = TRUE,
sep = '\t')
x <- read.table(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/test.txt",
header = TRUE,
sep = '\t')
x <- read.table(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/test_19_02.txt",
header = TRUE,
sep = '\t')
x <- read.table(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/test_19_02.txt",
header = TRUE,
sep = '\t')
?read.csv
library(devtools)
install_github("ropensci/rselenium")
library(RSelenium)
checkForServer()
startServer()
remDr <- remoteDriver()
remDr$open()
remDr$navigate('http://apps.webofknowledge.com/summary.do?product=UA&doc=1&qid=1&SID=N2oMhcraReuV3kWZ5aI&search_mode=AdvancedSearch&update_back2search_link_param=yes')
remDr$navigate("http://apps.webofknowledge.com/CitationReport.do?product=UA&search_mode=CitationReport&SID=4CvyYFKm3SC44hNsA2w&page=1&cr_pqid=7&viewType=summary")
x <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.csv",
header = TRUE,
sep = '\t')
View(x)
x <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
header = TRUE,
sep = '\t')
View(x)
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
header = TRUE,
sep = '\t')
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
header = TRUE,
sep = '\t')
str(q1)
q1$id <- nrow(q1)
q1$id
q1$id <- row(q1)
q1$id
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
header = TRUE,
sep = '\t')
str(q1)
mydf <- q1 %>% select(AU, AB)
library(dplyr)
mydf <- q1 %>% select(AU, AB)
head(mydf)
install.packages('tm')
library("tm")
a <- Corpus(VectorSource(mydf$AB))
a <- tm_map(a, tolower)
a <- tm_map(a, removePunctuation)
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english"))
require(rJava) # needed for stemming function
require(Snowball)
install.packages('Snowball')
install.packages("SnowballC")
require(SnowballC)
a <- tm_map(a, stemDocument, language = "english") # converts terms to tokens
a.tdm <- TermDocumentMatrix(a, control = list(minWordLength = 3)) # create a term document matrix, keepiing only tokens longer than three characters, since shorter tokens are very hard to interpret
inspect(a.tdm[1:10,1:10]) # have a quick look at the term document matrix
a <- Corpus(VectorSource(mydf$AB))
a <- tm_map(a, tolower)
a <- tm_map(a, removePunctuation)
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, removeWords, stopwords("english")) # this list needs to be edited and this function repeated a few times to remove high frequency context specific words with no semantic value
require(rJava) # needed for stemming function
require(SnowballC)
a <- tm_map(a, stemDocument, language = "english") # converts terms to tokens
a.tdm <- DocumentTermMatrix(a)  , control = list(minWordLength = 3))
a.tdm <- DocumentTermMatrix(a)
a <- Corpus(VectorSource(mydf$AB))
a <- tm_map(a, content_transformer(tolower))
a <- tm_map(a, removePunctuation)
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, removeWords, stopwords("english")) # this list needs to be edited and this function repeated a few times to remove high frequency context specific words with no semantic value
a.tdm <- DocumentTermMatrix(a)
a.tdm <- DocumentTermMatrix(a, control = list(minWordLength = 3))
head(a.tdm)
findFreqTerms(a.tdm, lowfreq=30)
findFreqTerms(a.tdm, lowfreq=100)
findFreqTerms(a.tdm, lowfreq=150)
findFreqTerms(a.tdm, lowfreq=30)
# Read table
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
header = TRUE,
sep = '\t')
summary(q1$PY)
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500.txt",
header = TRUE,
sep = '\t')
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500.txt",
header = TRUE,
sep = '\t')
summary(q1$PY)
machine <- '/User/ajpelu'
machine
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review/"", sep = "")
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review/", sep = "")
di
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "")
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "")
q0 <- read.csv(file="/data/Search_19_02_16_1_A_500.txt",
header = TRUE,
sep = '\t')
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "")
setwd(di)
getwd()
machine <- "/Users/ajpelu"
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "")
setwd(di)
q0 <- read.csv(file="/data/Search_19_02_16_1_A_500.txt",
header = TRUE,
sep = '\t')
# Read table
q0 <- read.csv(file=paste(di, "/data/Search_19_02_16_1_A_500.txt", sep= ""),
header = TRUE,
sep = '\t')
q0 <- read.csv(file=paste(di, "/data/Search_19_02_16_1_A_500.txt", sep= ""),
header = TRUE,
sep = '\t')
q1 <- read.csv(file=paste(di, "/data/Search_19_02_16_501_A_1000.txt", sep= ""),
header = TRUE,
sep = '\t')
q0 <- read.csv(file=paste(di, "/data/Search_19_02_16_1_A_500.txt", sep= ""),
header = TRUE,
sep = '\t')
q1 <- read.csv(file=paste(di, "/data/Search_19_02_16_501_A_1000.txt", sep= ""),
header = TRUE,
sep = '\t')
q2 <- read.csv(file=paste(di, "/data/Search_19_02_16_1001_A_1039.txt", sep= ""),
header = TRUE,
sep = '\t')
q <- rbind(q0,q1,2)
q <- rbind(q0,q1,q2)
str(q)
is.na(q$Z4)
colSums(is.na(q$Z4))
qt <- q[, !apply(is.na(q), 2, all)]
View(qt)
str(qt)
re <- read.csv(file=paste(di, "/data/RefTab.txt", sep= ""),
header = TRUE,
sep = '\t')
re <- read.csv(file=paste(di, "/data/RefTab.txt", sep= ""),
header = TRUE,
sep = '\t')
re <- read.csv(file=paste(di, "/data/wos1.txt", sep= ""),
header = TRUE,
sep = '\t')
View(re)
wos1 <- read.csv(file=paste(di, "/data/wos1.txt", sep= ""),
header = TRUE,
sep = '\t')
machine <- "/Users/ajpelu"
# machine <- "/Users/ajpeluLap"
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "")
wos1 <- read.csv(file=paste(di, "/data/wos1.txt", sep= ""),
header = TRUE,
sep = '\t')
wos2 <-  read.csv(file=paste(di, "/data/wos2.txt", sep= ""),
header = TRUE,
sep = '\t')
zoo <- read.csv(file=paste(di, "/data/zoo.txt", sep= ""),
header = TRUE,
sep = '\t')
bioCI <- read.csv(file=paste(di, "/data/bioCI.txt", sep= ""),
header = TRUE,
sep = '\t')
bioP <- read.csv(file=paste(di, "/data/bioP.txt", sep= ""),
header = TRUE,
sep = '\t')
med <- read.csv(file=paste(di, "/data/med.txt", sep= ""),
header = TRUE,
sep = '\t')
sci <- read.csv(file=paste(di, "/data/sci.txt", sep= ""),
header = TRUE,
sep = '\t')
sci <- read.csv(file=paste(di, "/data/sci.txt", sep= ""),
header = TRUE,
sep = '\t')
rawdf <- rbind(bioCI, bioP, med, wos1, wos2, zoo)
View(rawdf)
str(rawdf)
View(rawdf)
View(rawdf)
View(wos1)
str(wos1)
d <- rawdf[, c("Authors..Primary", "Author.Address")]
View(d)
