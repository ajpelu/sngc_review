---
title: "Frequency Analysis"
output: html_document
---

The purpose of this file is to develop a word frequency analysis for the analysis of the abstracts, so that we can later on compare them, relate them or whatever we want to do. 

For this analysis, we will use the complete table produced by the import from WoS, and the subsequent modification to merge all citations in one table and add the key (see "cleaning data" for more details).

# There is also another way to do it, with package "tm". Antonio started some testing with that one, in file "tm_package_test"

```{r}
machine <- "/Users/DELL" # Define machine for Maria
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "") # define directory for Maria

library(koRpus) # Frequency and lexical analysis
```

Tokenize: Splits the text into the components, no lemmatization is done (as TreeTagger does). Syntaxis:

tokenize(txt, format = "file", fileEncoding = NULL, split = "[[:space:]]",
  ign.comp = "-", heuristics = "abbr", heur.fix = list(pre = c("’",
  "'"), suf = c("’", "'")), abbrev = NULL, tag = TRUE, lang = "kRp.env",
  sentc.end = c(".", "!", "?", ";", ":"), detect = c(parag = FALSE, hline =
  FALSE), clean.raw = NULL, perl = FALSE, stopwords = NULL,
  stemmer = NULL)
  
txt: Either an open connection, the path to directory with txt files to read and tokenize, or a vector object already holding the text corpus.

object: Either "file" or "obj", depending on whether you want to scan files or analyze the given object

# Duda: como le digo que me coja el campo "Abstract" de la tabla "rawdf" y haga el tokenize para cada fila por separado?

```{r}
tagged.text <- tokenize("...")
```

```{r}
freq.analysis.res <- freq.analysis(tagged.text, corp.freq=NULL) # frequency analysis
freq.analysis<-taggedText(freq.analysis.res) # needed to be able to visualize the analysis in a table
View(freq.analysis) # visualization
```

#############################

Another option: Antonio's test with "tm package"

getwd()

# Read table 
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
              header = TRUE,
              sep = '\t')

str(q1)

# ver esto http://stackoverflow.com/questions/24771165/r-project-no-applicable-method-for-meta-applied-to-an-object-of-class-charact 

library(dplyr)

mydf <- q1 %>% select(AU, AB)

## text mining 
# install.packages('tm')
library("tm")

a <- Corpus(VectorSource(mydf$AB))
a <- tm_map(a, content_transformer(tolower))

a <- tm_map(a, removePunctuation) 
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, removeWords, stopwords("english")) # this list needs to be edited and this function repeated a few times to remove high frequency context specific words with no semantic value 
require(rJava) # needed for stemming function 
require(SnowballC)

a <- tm_map(a, stemDocument, language = "english") # converts terms to tokens
a.tdm <- DocumentTermMatrix(a, control = list(minWordLength = 3)) 

findFreqTerms(a.tdm, lowfreq=30)

# create a term document matrix, keepiing only tokens longer than three characters, since shorter tokens are very hard to interpret
inspect(a.tdm[1:10,1:10]) # have a quick look at the term document matrix
findFreqTerms(a.tdm, lowfreq=30) # have a look at common words, in this case, those that appear at least 30 times, good to get high freq words and add to stopword list and re-make the dtm, in this case add aaa, panel, session
findAssocs(a.tdm, 'scienc', 0.3) # find associated words and strength of the common words. I repeated this function for the ten most frequent words.
