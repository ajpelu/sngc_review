---
title: "Frequency Analysis"
output: html_document
---

The purpose of this file is to develop a word frequency analysis for the analysis of the abstracts, so that we can later on compare them, relate them or whatever we want to do. 

For this analysis, we will use the complete table produced by the import from WoS, and the subsequent modification to merge all citations in one table and add the key (see "cleaning data" for more details).

# Note: There is also another way to do it, with package "tm" (below).

# Set directory

```{r}
# Define machine for Maria
machine <- "/Users/DELL" 
# Maria en casa: machine<-"/Users/Maria"
machine <- "/Users/ajpeluLap"

# define directory for Maria
di <- paste(machine, "/Dropbox/Review_Sierra_Nevada/sngc_review", sep = "") 

# install.packages('koRpus')
library(koRpus) # Frequency and lexical analysis
```

# Import data and explore them

~~table<-read.table("raw_wos_with_key.txt",header=TRUE,sep="\t",quote = "",col.names = c("Reference.Type","Authors..Primary","Title.Primary","Periodical.Full","Periodical.Abbrev","Pub.Year","Pub.Date.Free.From","Volume","Issue","Start.Page","Other.Pages","Keywords","Abstract","Notes","Personal.Notes","Authors..Secondary","Title.Secondary","Edition","Publisher","Place.Of.Publication","Authors..Tertiary","Authors..Quaternary","Authors..Quinary","Title..Tertiary","ISSN.ISBN","Availability","Author.Address","Accession.Number","Language","Classification","Sub.file.Database","Original.Foreign.Title","Links","URL","DOI","PMID","PMCID","Call.Number","Database","Data.Source","Identifying.Phrase","Retrieved.Date","Shortened.Title","User.1","User.2","User.3","User.4","User.5","User.6","User.7","User.8","User.9","User.10","User.11","User.12","User.13","User.14","User.15","aut","key"))~~ For some reason if "quote" is not used it gives some warning message and does not recognize 1038 cites, but then it changes the column names, so col.names is used to keep the original names.

```{r}
tabla <- read.csv(file=paste0(di, "/raw_wos_with_key.txt"), header=TRUE, sep="\t")
  read.table(file=paste0(di, "/raw_wos_with_key.txt"), header=TRUE,sep="\t")

View(tabla)
str(tabla)
dim(tabla)
```

**Error en la importacion**: 366 filas, no reconoce correctamente los NA, aunque la str es correcta

# Tokenize

Tokenize: Splits the text into the components, no lemmatization is done (as TreeTagger does). Syntaxis:

tokenize(txt, format = "file", fileEncoding = NULL, split = "[[:space:]]",
  ign.comp = "-", heuristics = "abbr", heur.fix = list(pre = c("’",
  "'"), suf = c("’", "'")), abbrev = NULL, tag = TRUE, lang = "kRp.env",
  sentc.end = c(".", "!", "?", ";", ":"), detect = c(parag = FALSE, hline =
  FALSE), clean.raw = NULL, perl = FALSE, stopwords = NULL,
  stemmer = NULL)
  
txt: Either an open connection, the path to directory with txt files to read and tokenize, or a vector object already holding the text corpus.

object: Either "file" or "obj", depending on whether you want to scan files or analyze the given object

This is an example code, which does the freq. analysis, but cannot differentiate between different texts:

```{r}
tagged.text <- tokenize("...")

freq.analysis.res <- freq.analysis(tagged.text, corp.freq=NULL) # frequency analysis
freq.analysis<-taggedText(freq.analysis.res) # needed to be able to visualize the analysis in a table
View(freq.analysis) # visualization
```

So now we need to do that but for each abstract independently.

## Explore abstracts

```{r}
abstract=table$Abstract
n=length(abstract) 
n

mode(abstract) # "numeric"
typeof(abstract) # "integer"
typeof(abstract[2]) # "integer"
```

Once the exportation (rawdf) and importation (tabla) is correct, results should be:

n => 1038
mode(abstract) => 
typeof(abstract) => 
typeof(abstract[2]) => 

So we need to change the type of abstracts

for (i in 1:n) 
{
abstract[i]=as.character(abstract[i])
}

as(abstract[2], Class="character", strict=TRUE, ext)


Para que coja el campo "Abstract" de la tabla "rawdf" 

```{r}
for (i in 1:n) 
{
abstract_tokenized[i] = tokenize(abstract[i]) #provided that abstract is the right type
}
```




# abstract_tokenized=NULL # necessary? actually tokenize does not create something on its own, unless tag=TRUE is set.

#############################

Another option: Antonio's test with "tm package"

getwd()

# Read table 
q1 <- read.csv(file="/Users/ajpelu/Dropbox/Review_Sierra_Nevada/Data/Search_19_02_16_1_A_500_Campos_limitados.txt",
              header = TRUE,
              sep = '\t')

str(q1)

# ver esto http://stackoverflow.com/questions/24771165/r-project-no-applicable-method-for-meta-applied-to-an-object-of-class-charact 

library(dplyr)

mydf <- q1 %>% select(AU, AB)

## text mining 
# install.packages('tm')
library("tm")

a <- Corpus(VectorSource(mydf$AB))
a <- tm_map(a, content_transformer(tolower))

a <- tm_map(a, removePunctuation) 
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, removeWords, stopwords("english")) # this list needs to be edited and this function repeated a few times to remove high frequency context specific words with no semantic value 
require(rJava) # needed for stemming function 
require(SnowballC)

a <- tm_map(a, stemDocument, language = "english") # converts terms to tokens
a.tdm <- DocumentTermMatrix(a, control = list(minWordLength = 3)) 

findFreqTerms(a.tdm, lowfreq=30)

# create a term document matrix, keepiing only tokens longer than three characters, since shorter tokens are very hard to interpret
inspect(a.tdm[1:10,1:10]) # have a quick look at the term document matrix
findFreqTerms(a.tdm, lowfreq=30) # have a look at common words, in this case, those that appear at least 30 times, good to get high freq words and add to stopword list and re-make the dtm, in this case add aaa, panel, session
findAssocs(a.tdm, 'scienc', 0.3) # find associated words and strength of the common words. I repeated this function for the ten most frequent words.
